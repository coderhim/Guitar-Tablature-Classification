# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# from transformers import ViTModel, ViTImageProcessor

# class ViTGuitarTabModel(nn.Module):
#     def __init__(self, num_classes=19, dropout_rate=0.3, pretrained_model="facebook/dino-vits8"):
#         super(ViTGuitarTabModel, self).__init__()
        
#         # Load pre-trained ViT model & processor
#         self.processor = ViTImageProcessor.from_pretrained(pretrained_model)
#         self.vit = ViTModel.from_pretrained(pretrained_model)
        
#         # Adjust hidden size for dino-vit-small (384 for small models)
#         vit_output_dim = self.vit.config.hidden_size  # 384 for dino-vit-small

#         # Shared layers after ViT backbone
#         self.dropout1 = nn.Dropout(dropout_rate)
#         self.fc1 = nn.Linear(vit_output_dim, 512)
#         self.bn_fc1 = nn.BatchNorm1d(512)
#         self.dropout2 = nn.Dropout(dropout_rate)
#         self.fc2 = nn.Linear(512, 256)
#         self.bn_fc2 = nn.BatchNorm1d(256)
        
#         # String-specific heads (one for each guitar string)
#         self.string_heads = nn.ModuleList([
#             nn.Sequential(
#                 nn.Dropout(dropout_rate / 2),  # Less dropout in final layers
#                 nn.Linear(256, num_classes)
#             ) for _ in range(6)  # 6 guitar strings
#         ])

#         # Initialize custom layers
#         self._init_custom_weights()

#     def _init_custom_weights(self):
#         # Initialize weights for the fully connected layers
#         for m in [self.fc1, self.fc2]:
#             nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')
#             if m.bias is not None:
#                 nn.init.constant_(m.bias, 0)

#         for m in [self.bn_fc1, self.bn_fc2]:
#             nn.init.constant_(m.weight, 1)
#             nn.init.constant_(m.bias, 0)

#         # Initialize string head layers
#         for head in self.string_heads:
#             for layer in head:
#                 if isinstance(layer, nn.Linear):
#                     nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='leaky_relu')
#                     if layer.bias is not None:
#                         nn.init.constant_(layer.bias, 0)

#     def reshape_for_vit(self, x):
#         """Ensure input is resized and formatted for ViT"""
#         # If input is [batch, time, freq], add channel dimension
#         if x.dim() == 3:
#             x = x.unsqueeze(1)  # [batch, 1, time, freq]
        
#         # Resize to 128x128 (for dino-vit-small-patch8)
#         if x.shape[2] != 128 or x.shape[3] != 128:
#             x = F.interpolate(x, size=(128, 128), mode='bicubic', align_corners=False)
        
#         # ViT can work with 1 channel inputsâ€”no need to repeat
#         return x

#     def forward(self, x):
#         # Preprocess and reshape for ViT
#         # x = self.reshape_for_vit(x)
#         batch_size = x.shape[0]
#         # if x.shape[1] == 1:  # Single channel
#         #     x = x.repeat(1, 3, 1, 1)  # Repeat to 3 channels
#         # Process with ViT
#         # inputs = self.processor(images=x, return_tensors="pt")
    
#         # Print shape information for debugging
#         # print(f"Input tensor shape: {x.shape}")
        
#         # # Normalize data to [0, 1] range (if not already done in preprocessing)
#         # x_normalized = (x + 120) / 120
#         # x_normalized = torch.clamp(x_normalized, 0, 1)
#         outputs = self.vit(pixel_values=x)

#         # Extract [CLS] token representation
#         x = outputs.last_hidden_state[:, 0]  # Shape: [batch_size, hidden_size]

#         # Pass through shared fully connected layers
#         x = self.dropout1(x)
#         x = F.leaky_relu(self.bn_fc1(self.fc1(x)), negative_slope=0.1)
#         x = self.dropout2(x)
#         x = F.leaky_relu(self.bn_fc2(self.fc2(x)), negative_slope=0.1)

#         # Apply string-specific heads
#         outputs = [head(x) for head in self.string_heads]

#         return outputs


import torch
import torch.nn as nn
from transformers import AutoModel

class DinoGuitarTabModel(nn.Module):
    def __init__(self, num_classes=19):
        super(DinoGuitarTabModel, self).__init__()
        # Use facebook's small ViT-like Dino model
        self.encoder = AutoModel.from_pretrained("facebook/dinov2-small")

        # Extract output size dynamically
        vit_output_dim = self.encoder.config.hidden_size

        # Head for each guitar string
        self.string_heads = nn.ModuleList([
            nn.Sequential(
                nn.Linear(vit_output_dim, 256),
                nn.ReLU(),
                nn.Linear(256, num_classes)
            ) for _ in range(6)  # 6 guitar strings
        ])

    def forward(self, x):
        x = self.encoder(x).last_hidden_state[:, 0]  # [CLS] token
        return [head(x) for head in self.string_heads]

# model = DinoGuitarTabModel()
